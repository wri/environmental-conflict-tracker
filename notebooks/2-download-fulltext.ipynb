{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: John Brandt\n",
    "\n",
    "Scrape fulltext of news articles associated with each geolocated event in [GDELT](gdeltproject.org) for a given date.\n",
    "\n",
    "Creates the following output:\n",
    "\n",
    "* `{country}/metadata/{year}/{month}.csv`: `pd.DataFrame` with one row per candidate event and attached GDELT metadata;\n",
    "* `{country}/metadata/{year}/{month}.pkl`: `Dict` matching each row of `{month}.csv` to one or more `unique_id` in `/text/`\n",
    "* `{country}/text/{year}/{unique_id}.pkl`: `JSON` of article full text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Package imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NewsPlease](https://github.com/fhamborg/news-please/tree/master/newsplease) is used to scrape news articles. Tensorflow is used for easy IO structures with GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsplease import NewsPlease\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tnrange\n",
    "import re\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Download\n",
    "\n",
    "## 2.0 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words = ['land', 'forest', 'agriculture', \n",
    "                  'farm', 'farmer', 'plantation', 'agrarian',\n",
    "                  'smallholder', 'grazing', 'development', 'habitat', \n",
    "                  'resource', 'cattle', 'dispute', 'strife', 'peat',\n",
    "                  'rice', 'palm oil', 'sugarcane', 'cassava', 'coconut',\n",
    "                  'corn', 'mango', 'orange', 'maize', 'wheat', 'sorghum',\n",
    "                  'bananas', 'tomatoes', 'citrus',\n",
    "                  'livestock', 'kill', 'dead', 'airport',\n",
    "                  'aluminum', 'mining', 'agro', 'dam',\n",
    "                  'road', 'infrastructure', 'transmission', \n",
    "                  'conservation', 'settlement', 'displace',\n",
    "                  'exile', 'caste', 'conflict', 'relocation',\n",
    "                  'village', 'encroach', 'fertilizer', 'mine',\n",
    "                  'illegal mining', 'malnutrition', 'contamination',\n",
    "                  'mangrove', 'water', 'cow', 'cattle', 'appropriation', \n",
    "                  'appropriated', 'protest', 'environmental', 'pollution',\n",
    "                  'copper', 'iron', 'timber', 'acre', 'hectare', ]\n",
    "\n",
    "days_per_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "country = 'brazil'\n",
    "month = 5\n",
    "\n",
    "input_folder = \"../data/{}/raw/{}/\".format(country, str(year))\n",
    "metadata_folder = \"../data/{}/metadata/{}/\".format(country, str(year))\n",
    "text_output_folder = \"../data/{}/text/{}/{}/\".format(country, str(year), str(month).zfill(2))\n",
    "\n",
    "for folder in [input_folder, metadata_folder, text_output_folder]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_month(country: str,\n",
    "             year: int,\n",
    "             month: int) -> pd.DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Loads the data downloaded in 1-download-gdelt for each day in input\n",
    "    month and year for input country, returns a list of dataframes.\n",
    "    \n",
    "    Parameters\n",
    "     country (str): either of 'brazil', 'indonesia', 'mexico'\n",
    "     year (int): either of [2017, 2018, 2019]\n",
    "     month (int): calendar month as integer\n",
    "     \n",
    "    Returns\n",
    "     dfs (list): list of pandas DataFrames\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    for day in range(days_per_month[month + 1]): \n",
    "        file = (input_folder + str(year) + str(month).zfill(2) \n",
    "                + str(day).zfill(2) + \".csv\")\n",
    "        if os.path.exists(file):\n",
    "            file = pd.read_csv(file)\n",
    "            dfs.append(file)\n",
    "    return dfs\n",
    "\n",
    "def find_link_to_scrape(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Identifies unique links to scrape for a single dataframe\n",
    "    '''\n",
    "    \n",
    "    df['to_scrape'] = ''\n",
    "    df['title'] = ''\n",
    "    for i in range(len(df)):\n",
    "        links = df['SOURCEURL'][i]\n",
    "        l = re.findall(r'\\w+(?:-\\w+)+', links)\n",
    "        if l:\n",
    "            title = max(l, key = len)\n",
    "            title = title.replace('-', ' ')\n",
    "            df['title'][i] = title\n",
    "            if any(word in title for word in relevant_words):\n",
    "                df['to_scrape'][i] = str(links)\n",
    "    return df\n",
    "\n",
    "def combine_days(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Combines a list of dataframes and returns a reset index\n",
    "    '''\n",
    "    df_parsed = [find_link_to_scrape(dfs[x]) for x in tnrange(len(dfs))]\n",
    "    df_month = pd.concat(df_parsed)\n",
    "    df_subs = df_month[df_month['to_scrape'] != '']\n",
    "    df_subs = df_subs.reset_index()\n",
    "    return df_subs\n",
    "\n",
    "def save_obj(obj: Any, name: str, folder: str) -> None:\n",
    "    'Helper function using pickle to save and load objects'\n",
    "    with open(folder + name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def download_url(url: str) -> None:\n",
    "    try:\n",
    "        article = NewsPlease.from_url(urls[url])\n",
    "        save_obj(article, str(url).zfill(5), text_output_folder)\n",
    "        return 1\n",
    "    except Exception as ex:\n",
    "        print(url, ex)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata for the input month, which is processed to become a list of dataframes, where each row corresponds to a candidate news event referencing environmental conflict. Save this csv to the metadata folder, named by the year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_month(country, year, month)\n",
    "df = combine_days(df)\n",
    "df.to_csv(metadata_folder + str(month).zfill(2) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each news article may reference more than one event (e.g. \"Farmers protested because of a recent death due to malnutrition\" contains both an event of protest and an event of malnutrition), a hashing table or dictionary is needed to keep track of the references.\n",
    "\n",
    "`mapping_dictionary` is made such that the keys are the `unique_id`, and the values are a list of `pd.Index` items for the attached `pd.DataFrame`. This dictionary gets saved as a `.pkl` file in the `metadata_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df['to_scrape'].unique()\n",
    "mapping_dictionary = {}\n",
    "for i, val in enumerate(urls):\n",
    "    match = df.index[df['to_scrape'] == urls[i]].tolist()\n",
    "    mapping_dictionary[i] = match \n",
    "    \n",
    "save_obj(mapping_dictionary, str(month).zfill(2), metadata_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the full text for each url in the `df`, saving it to a `.pkl` file in `text_output_folder` with a name corresponding to the associated `df` row and `matching_dictionary` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_texts = [for x in os.listdir(text_output_folder) if \".DS\" not in x]\n",
    "to_download = [x for x in range(0, len(urls)) if x not in existing_texts]\n",
    "\n",
    "pool = multiprocessing.Pool(16)\n",
    "zip(*pool.map(download_url, to_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
